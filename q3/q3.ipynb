{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b829d90c",
   "metadata": {},
   "source": [
    "# Toxic Comment Classicfication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d675383c",
   "metadata": {},
   "source": [
    "Installing the scikit-learn (sklearn) library <br>\n",
    "Importing numpy, pandas and matplotlib for data processing and plotting <br>\n",
    "Importing re and string to operate with strings and regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed730df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd480ba",
   "metadata": {},
   "source": [
    "Reading the input files from the csv format (after manually unzipping them) and printing the column labels using `train.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bbaa210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "subm = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02196d7",
   "metadata": {},
   "source": [
    "Creating a column for non-toxic comments which is supposedly the vast majority as can be inferred from the `train.describe()` data of `mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66513773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>non-toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.898321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate      non-toxic  \n",
       "count  159571.000000  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805       0.898321  \n",
       "std         0.216627       0.093420       0.302226  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       1.000000  \n",
       "50%         0.000000       0.000000       1.000000  \n",
       "75%         0.000000       0.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['non-toxic'] = 1-train[label_cols].max(axis=1)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec38d85",
   "metadata": {},
   "source": [
    "Checking whether there exist any null valued columns/comment texts so we can ignore them while predicting (Turns out the data is clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3523916d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(id               0\n",
       " comment_text     0\n",
       " toxic            0\n",
       " severe_toxic     0\n",
       " obscene          0\n",
       " threat           0\n",
       " insult           0\n",
       " identity_hate    0\n",
       " non-toxic        0\n",
       " dtype: int64,\n",
       " id              0\n",
       " comment_text    0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum(), test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600dd319",
   "metadata": {},
   "source": [
    "The function `token` is used to find all the strings containing punctuations and replacing them with a blank space and then splitting the regex object. This regex object is created using the `re.compile()` function. <br><br>\n",
    "\n",
    "The `TfifVectorizer` method imported from `sklearn` works in the following way:\n",
    "- The `ngram_range` defines the max and min sizes of phrases that are to be used in the vocabulary. (Here it is either 1-word or 2-word phrases).\n",
    "- The `tokenizer` creates these words by using the defined function `token`\n",
    "- The `min_df=3` ignores the words in the vocabulary that are used less than 3 times and `max_df=0.9` ignores the words that form more than 90% of the total document, i.e., too frequently used to be helpful for our model.\n",
    "- `strip_accents` has a similar job as that of the tokenizer, it strips down all the <b>'unicode'</b> type characters which is basically an exteneded <b>'ascii'</b>.\n",
    "- `sublinear_tf` works as a boolean of how we scale the freqeuncy of vocabulary to the significance. Assigning it true we use a sublinear logarithmic function which assigns weights to the frequencies. \n",
    "\n",
    "And now we call it on the <b>\"comment_text\"</b> column for both the training and testing data. <br>\n",
    "<u><b>Note:</b></u> We use `fit_transform()` instead of `transform()` as it automatically scales the data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4dceba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<159571x425924 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 17758669 stored elements in Compressed Sparse Row format>,\n",
       " <153164x425924 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 14751682 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token(s):\n",
    "    return re.compile(f'([{string.punctuation}])').sub(r' \\1 ', s).split()\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=token, min_df=3, max_df=0.9, strip_accents='unicode', sublinear_tf=True )\n",
    "X = vec.fit_transform(train['comment_text'])\n",
    "test_X = vec.transform(test['comment_text'])\n",
    "\n",
    "X, test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c0afa",
   "metadata": {},
   "source": [
    "Now we define functions:\n",
    "- `bayes` which takes in the value at an index(`Y_index`) and the scaled frequencies(`Y`) which serve as probabilities of occurence and apply the naive Bayes equation on the sparse matrix `X` with only a few non-zero entries.\n",
    "- `get_model` functions returns a model fit using Logarithmic regression on a given input of training data column using the default `solver = lgbfs` for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf7b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes(Y_index, Y):\n",
    "    p = X[Y==Y_index].sum(0)\n",
    "    return (p+1) / ((Y==Y_index).sum()+1)\n",
    "\n",
    "def get_model(Y):\n",
    "    Y = Y.values\n",
    "    r = np.log(bayes(1,Y) / bayes(0,Y))\n",
    "    m = LogisticRegression(C=4, max_iter=10000)\n",
    "    X_nb = X.multiply(r)\n",
    "    return m.fit(X_nb, Y), r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a90b8d",
   "metadata": {},
   "source": [
    "We first run the each solumn of our training data on the `get_model` function which returns the model fit for the `train` and the logarithmic scale `r`.<br> \n",
    "Then we run `m.predict_proba()` on the testing data and store it in the <b>submission.csv</b> file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8035ffa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.109407</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.963042</td>\n",
       "      <td>0.095380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.011778</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.005566</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.000291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.009949</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.000339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>0.586452</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.068315</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.018617</td>\n",
       "      <td>0.000366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>0.017819</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.019616</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.018754</td>\n",
       "      <td>0.001581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>0.008045</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.000863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>0.945596</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.429715</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.044527</td>\n",
       "      <td>0.000793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id     toxic  severe_toxic   obscene    threat  \\\n",
       "0       00001cee341fdb12  0.999988      0.109407  0.999987  0.002334   \n",
       "1       0000247867823ef7  0.002865      0.000588  0.001887  0.000089   \n",
       "2       00013b17ad220c46  0.011778      0.000843  0.005566  0.000090   \n",
       "3       00017563c3f7919a  0.000955      0.000220  0.001140  0.000157   \n",
       "4       00017695ad8997eb  0.009949      0.000464  0.001992  0.000115   \n",
       "...                  ...       ...           ...       ...       ...   \n",
       "153159  fffcd0960ee309b5  0.586452      0.000295  0.068315  0.000110   \n",
       "153160  fffd7a9a6eb32c16  0.017819      0.001031  0.019616  0.001074   \n",
       "153161  fffda9e8d6fafa9e  0.001381      0.000154  0.002696  0.000070   \n",
       "153162  fffe8f1340a79fc2  0.008045      0.000329  0.002248  0.000087   \n",
       "153163  ffffce3fb183ee80  0.945596      0.000114  0.429715  0.000318   \n",
       "\n",
       "          insult  identity_hate  \n",
       "0       0.963042       0.095380  \n",
       "1       0.002215       0.000330  \n",
       "2       0.003255       0.000288  \n",
       "3       0.001051       0.000291  \n",
       "4       0.002369       0.000339  \n",
       "...          ...            ...  \n",
       "153159  0.018617       0.000366  \n",
       "153160  0.018754       0.001581  \n",
       "153161  0.000937       0.000187  \n",
       "153162  0.002192       0.000863  \n",
       "153163  0.044527       0.000793  \n",
       "\n",
       "[153164 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.zeros((len(test), len(label_cols)))\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    m,r = get_model(train[j])\n",
    "    preds[:,i] = m.predict_proba(test_X.multiply(r))[:,1]\n",
    "\n",
    "submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
